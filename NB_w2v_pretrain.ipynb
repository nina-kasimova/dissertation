{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "roman-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "partial-warrior",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./GoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1629\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1585\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1586\u001b[0m     ):\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1972\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1969\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m-> 1972\u001b[0m     \u001b[43m_word2vec_read_binary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1867\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1865\u001b[0m new_chunk \u001b[38;5;241m=\u001b[39m fin\u001b[38;5;241m.\u001b[39mread(binary_chunk_size)\n\u001b[1;32m   1866\u001b[0m chunk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chunk\n\u001b[0;32m-> 1867\u001b[0m processed_words, chunk \u001b[38;5;241m=\u001b[39m \u001b[43m_add_bytes_to_kv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1869\u001b[0m tot_processed_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m processed_words\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_chunk) \u001b[38;5;241m<\u001b[39m binary_chunk_size:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1853\u001b[0m, in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)\u001b[0m\n\u001b[1;32m   1851\u001b[0m word \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1852\u001b[0m vector \u001b[38;5;241m=\u001b[39m frombuffer(chunk, offset\u001b[38;5;241m=\u001b[39mi_vector, count\u001b[38;5;241m=\u001b[39mvector_size, dtype\u001b[38;5;241m=\u001b[39mREAL)\u001b[38;5;241m.\u001b[39mastype(datatype)\n\u001b[0;32m-> 1853\u001b[0m \u001b[43m_add_word_to_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1854\u001b[0m start \u001b[38;5;241m=\u001b[39m i_vector \u001b[38;5;241m+\u001b[39m bytes_per_vector\n\u001b[1;32m   1855\u001b[0m processed_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1833\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary file is incomplete: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is missing\u001b[39m\u001b[38;5;124m\"\u001b[39m, word)\n\u001b[1;32m   1832\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m \u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vecattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:326\u001b[0m, in \u001b[0;36mKeyedVectors.set_vecattr\u001b[0;34m(self, key, attr, val)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(target_size, dtype\u001b[38;5;241m=\u001b[39mprev_expando\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr][: \u001b[38;5;28mmin\u001b[39m(prev_count, target_size), ] \u001b[38;5;241m=\u001b[39m prev_expando[: \u001b[38;5;28mmin\u001b[39m(prev_count, target_size), ]\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vecattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, attr, val):\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallocate_vecattrs(attrs\u001b[38;5;241m=\u001b[39m[attr], types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mtype\u001b[39m(val)])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = pd.read_csv('../code/emo_context_train.txt', sep=\"\\t\")\n",
    "class_counts = dialogues[\"label\"].value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print(\"class counts: \",class_counts)\n",
    "dialogues = dialogues[dialogues[\"label\"]!=\"others\"]\n",
    "labels = dialogues.label\n",
    "cols = [\"turn1\", \"turn2\", \"turn3\"]\n",
    "dialogues[\"combined\"] = dialogues[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "dialogues[\"combined\"] = dialogues[\"combined\"].apply(str.lower).apply(word_tokenize).apply(lambda x: \" \".join(x))\n",
    "dialogues[\"seq_len\"] = dialogues[\"combined\"].apply(len)\n",
    "print(dialogues.columns)\n",
    "\n",
    "max_len = dialogues[\"seq_len\"].max()\n",
    "input_text = dialogues.combined.values\n",
    "tokenized = []\n",
    "for sen in input_text:\n",
    "    tokenized.append(sen.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenized\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(tokenized, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized, y, test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = []\n",
    "for sentence in X_train:\n",
    "    sentence_vec = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model:\n",
    "            sentence_vec.append(word2vec_model[word])\n",
    "    train_vectors.append(np.mean(sentence_vec, axis=0))\n",
    "    \n",
    "test_vectors = []\n",
    "for sentence in X_test:\n",
    "    sentence_vec = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model:\n",
    "            sentence_vec.append(word2vec_model[word])\n",
    "    test_vectors.append(np.mean(sentence_vec, axis=0))\n",
    "    \n",
    "print(len(test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: \",accuracy_score(y_test, predictions))\n",
    "print(\"F1 score: \",f1_score(y_test, predictions, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6599b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
