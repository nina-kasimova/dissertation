{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56934523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'turn1', 'turn2', 'turn3', 'label', 'combined', 'seq_len'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def filter_stop_words(text: List[str])->List[str]:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "dialogues = pd.read_csv('../code/emo_context_train.txt', sep=\"\\t\")\n",
    "dialogues = dialogues[dialogues[\"label\"]!=\"others\"]\n",
    "labels = dialogues.label\n",
    "cols = [\"turn1\", \"turn2\", \"turn3\"]\n",
    "dialogues[\"combined\"] = dialogues[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "#dialogues[\"combined\"] = dialogues[\"combined\"].apply(str.lower).apply(word_tokenize).apply(filter_stop_words).apply(lambda x: \" \".join(x))\n",
    "dialogues[\"combined\"] = dialogues[\"combined\"].apply(str.lower).apply(word_tokenize).apply(lambda x: \" \".join(x))\n",
    "dialogues[\"seq_len\"] = dialogues[\"combined\"].apply(len)\n",
    "print(dialogues.columns)\n",
    "\n",
    "max_len = dialogues[\"seq_len\"].max()\n",
    "input_text = dialogues.combined.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f914d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 00:36:57.708526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import metrics\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import keras_tuner as kt\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8161e83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        2\n",
      "3        2\n",
      "10       2\n",
      "13       1\n",
      "15       0\n",
      "        ..\n",
      "30146    0\n",
      "30148    2\n",
      "30150    1\n",
      "30152    2\n",
      "30156    1\n",
      "Name: label, Length: 15212, dtype: int64\n",
      "<class 'numpy.ndarray'>\n",
      "15212\n"
     ]
    }
   ],
   "source": [
    "X = input_text\n",
    "label_map = {'happy':0,'sad':1,'angry':2}\n",
    "y = dialogues['label'].map(label_map)\n",
    "\n",
    "y = np.asarray(y).astype('float32')\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0332deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 67, 128)           256000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 67, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                6304      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 517,434\n",
      "Trainable params: 517,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fbcb5865900>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    embed_dim = 128\n",
    "    lstm_out = 196\n",
    "    max_features = 2000\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    activation=hp.Choice(\"activation\", [\"relu\", \"softmax\"])\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(units=hp_units, activation='softmax'))\n",
    "    model.add(Dense(10))\n",
    "    #model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "build_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc748a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fd21f702350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'embed_dim': [32, 64, 128],\n",
    "    'lstm_out': [32, 64, 128],\n",
    "    'dense_units': [16, 32, 64],\n",
    "    'dropout_rate': [0.2, 0.4, 0.6],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model1(hp):\n",
    "    max_features = 2000\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, hp.Int('embed_dim', min_value=32, max_value=128, step=32), input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(hp.Float('dropout_rate', min_value=0.2, max_value=0.6, step=0.1)))\n",
    "    model.add(LSTM(hp.Int('lstm_out', min_value=32, max_value=128, step=32), dropout=0.2, recurrent_dropout=0.2))\n",
    "    activation=hp.Choice(\"activation\", [\"relu\", \"softmax\"])\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=16, max_value=64, step=16), activation=activation))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')), metrics=['accuracy'])\n",
    "    return model\n",
    "                         \n",
    "build_model1(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad85d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model1,\n",
    "                     objective='accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     overwrite=True,\n",
    "                     directory='dir',\n",
    "                     project_name='project')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d6345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 03m 24s]\n",
      "accuracy: 0.9727175831794739\n",
      "\n",
      "Best accuracy So Far: 0.9777302742004395\n",
      "Total elapsed time: 00h 35m 38s\n",
      "\n",
      "Search: Running Trial #28\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |128               |embed_dim\n",
      "0.5               |0.2               |dropout_rate\n",
      "128               |64                |lstm_out\n",
      "softmax           |softmax           |activation\n",
      "64                |32                |dense_units\n",
      "0.0076576         |0.0019265         |learning_rate\n",
      "10                |10                |tuner/epochs\n",
      "0                 |4                 |tuner/initial_epoch\n",
      "0                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Epoch 1/10\n",
      "381/381 [==============================] - 28s 70ms/step - loss: 0.7536 - accuracy: 0.6637\n",
      "Epoch 2/10\n",
      "156/381 [===========>..................] - ETA: 16s - loss: 0.3230 - accuracy: 0.8974"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n",
    "batch_size = 32\n",
    "tuner.search(X_train, y_train, epochs=5)\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc2652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
